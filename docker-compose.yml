version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
    environment:
      - PYTHONPATH=/app
      - AI_FALLBACK_MODE=true

  # AI service commented out since we're using fallback mode
  # Uncomment and configure properly when GPU support is available
  # ai-service:
  #   image: ghcr.io/huggingface/text-generation-inference:1.1.0
  #   ports:
  #     - "8080:80"
  #   volumes:
  #     - ai-model-data:/data
  #   environment:
  #     - MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B
  #     - QUANTIZE=bitsandbytes
  #     - MAX_INPUT_LENGTH=4096
  #     - MAX_TOTAL_TOKENS=8192
  #     - USE_CUDA=true
  #     - CUDA_VISIBLE_DEVICES=0

volumes:
  ai-model-data:
